{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://www.univ-tlse3.fr/\" ><img src=\"http://www.univ-tlse3.fr/medias/photo/ut3pres_logoq_1372757033342.jpg?ID_FICHE=49702\" style=\"float:right; max-width: 250px; display: inline\" alt=\"INSA\"/></a> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Biais et Discrimination en Apprentissage Statistique\n",
    "## Correction du biais  sur les données `Adult Income` avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>\n",
    "\n",
    "### Résumé\n",
    "Utilisation des fonctions du [dépôt](https://github.com/algofairness/fairness-comparison) de  [Friedler et al. 2019](https://dl.acm.org/citation.cfm?id=3287589) pour tester différents outils de correction des biais d'apprentissage. Comparaison avec les résultats obtenus avec R.\n",
    "\n",
    "## 1 Introduction\n",
    "### 1.1 Outils\n",
    "Friedler et al. (2019) proposent une comparaison systématique de plusieurs algorithmes de réparation des données ou d'apprentissage loyal sur 5 jeux de données classiques: *Adult Income, Ricci, German Bank, Propublica recidivism, Propublica violent recidivism*. Dans la discussion finale, les auteurs insistent lourdement sur les choix opérés lors du prétraitement des données, la prolifération des mesures statistiques de biais et l'instabilité de la phase d'apprentissage. \n",
    "- Le prétraitement doit être rigoureusement identique avant l'application de différents algorithmes pour pouvoir en comparer les performances.\n",
    "- Il suffit de se limiter à quelques mesures de biais car celles-ci sont fortement corrélées . Prendre par exemple en compte l'effet disproportionné (*disparate impact* ou *DI*) et une comparaison des taux d'erreur conditionnels.\n",
    "- Il est important de reproduire les résultats sur plusieurs séparations aléatoires apprentissage - test des données.\n",
    "\n",
    "    Friedler et al. (2019) analysent donc les résultats de 10 exécutions opérées sur 5 jeux de données et pour une liste de 19 algorithmes combinant différents apprentissages (naïf bayésien, régression logistique, SVM, arbre de décision) et correction (Calders, Zafar, Kamishima, Feldman) ou pas du biais. Par souci de reproductibilité de la recherche, le code python est disponible dans un [dépôt](https://github.com/algofairness/fairness-comparison) public. Il peut être installé avec la commande: \n",
    "    \n",
    "   **`pip3 install fairness`**\n",
    "\n",
    "### 1.2 Objectifs\n",
    "*Attention* ce dépôt n'est pas une librairie comme peut l'être `scikit-learn` mais le code des programmes exécutant tous les prétraitements, les comparaisons de performances et la production des graphiques. En conséquence, l'exécution totale est excessivement longue et ne présente que peu d'intérêt. En revanche et dans l'attente de la réalisation d'une librairie efficace, il peut être utile ou intéressant de pouvoir mettre en oeuvre ces ressources comme pour produire les diagnostics et résultats d'un algorithme sur un jeu de données spécifique. C'est ce qui est tenté ci-dessous. Les résultats sont comparés avec ceux de l'article ainsi que ceux obtenus avec R.\n",
    "\n",
    "### 1.3 Problèmes rencontrés\n",
    "- Prétraitements: les auteurs ont à coeur de produire des codes génériques suffisamment généraux pour être exécutables sans intervention manuelle experte sur les données. C'est sûrement très positif pour une plus grande automatisation et standadisation des codes mais ne corrige pas des incohérences, erreurs de codification... pouvant être présentes et en fait toujours présentes dans des données réelles. D'autre part ils ne procèdent pas à des regroupements de modalités, par exemple en lien avec l'origine des individus. Certaines variables, dont celle sensible d'origine, possèdent un nombre important de modalités avec certaines de faible effectif donc sans réel intérêt. Cela complique inutilement les traitements et la production de résultats.\n",
    "- Ce sont des codes itératifs exécutant sytématiquement toutes les combinaisons des paramètres. Exécution très longue, il serait préférable de pouvoir faire des choix (données algorithme, paramètre) mais les fonctions ne sont pas documentées et donc difficilement adaptables à des besoins spécifiques.\n",
    "- Problème de mise à jour et compatibilité de certaines librairies entraînant la production de nombreux *warning* et des erreurs d'exécution pour certains algorithmes; tous les résultats ne peuvent pas être reproduits.\n",
    "- Volume des résultats: les objectifs de l'article conduivent à calculer tous les critères possibles pour finalement montrer que ceux-ci sont très liés et que quelques uns suffisent à caractériser les biais et la qualité de prévision; extraire les plus pertinents des fichiers de sortie n'est pas si simple.\n",
    "\n",
    "\n",
    "Les codes exécutés ci-dessous sont extraits du [dépôt](https://github.com/algofairness/fairness-comparison) associé à  l'article de Friedler et al. (2019).\n",
    "## 2 Prétraitements\n",
    "Les codes ci-dessous permettent de transformer les données initiales des 5 fichiers pour les rendre compatibles avec les traitements suivants. il s'agit avant tout de recoder les variables qualitatives, éventuellement celles quantitatives et de supprimer toutes les observations avec des données manquantes.\n",
    "### 2.1  Librairie et fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness.data.objects.list import DATASETS, get_dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_names = get_dataset_names()):\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        if not dataset.get_dataset_name() in dataset_names:\n",
    "            continue\n",
    "        print(\"--- Processing dataset: %s ---\" % dataset.get_dataset_name())\n",
    "        data_frame = dataset.load_raw_dataset()\n",
    "        d = preprocess(dataset, data_frame)\n",
    "        \n",
    "        for k, v in d.items():\n",
    "            write_to_file(dataset.get_filename(k), v)\n",
    "\n",
    "def write_to_file(filename, dataframe):\n",
    "    print(\"Writing data to: %s\" % filename)\n",
    "    dataframe.to_csv(filename, index = False)\n",
    "\n",
    "def preprocess(dataset, data_frame):\n",
    "    \"\"\"\n",
    "    The preprocess function takes a pandas data frame and returns two modified data frames:\n",
    "    1) all the data as given with any features that should not be used for training or fairness\n",
    "    analysis removed.\n",
    "    2) only the numerical and ordered categorical data, sensitive attributes, and class attribute.\n",
    "    Categorical attributes are one-hot encoded.\n",
    "    3) the numerical data (#2) but with a binary (numerical) sensitive attribute\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any columns not included in the list of features to keep.\n",
    "    smaller_data = data_frame[dataset.get_features_to_keep()]\n",
    "\n",
    "    # Handle missing data.\n",
    "    missing_processed = dataset.handle_missing_data(smaller_data)\n",
    "\n",
    "    # Remove any rows that have missing data.\n",
    "    missing_data_removed = missing_processed.dropna()\n",
    "    missing_data_count = missing_processed.shape[0] - missing_data_removed.shape[0]\n",
    "    if missing_data_count > 0:\n",
    "        print(\"Missing Data: \" + str(missing_data_count) + \" rows removed from dataset \" +  \\\n",
    "              dataset.get_dataset_name())\n",
    "\n",
    "    # Do any data specific processing.\n",
    "    processed_data = dataset.data_specific_processing(missing_data_removed)\n",
    "\n",
    "    print(\"\\n-------------------\")\n",
    "    print(\"Balance statistics:\")\n",
    "    print(\"\\nClass:\")\n",
    "    print(dataset.get_class_balance_statistics(processed_data))\n",
    "    print(\"\\nSensitive Attribute:\")\n",
    "    for r in dataset.get_sensitive_attribute_balance_statistics(processed_data):\n",
    "        print(r)\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Handle multiple sensitive attributes by creating a new attribute that's the joint distribution\n",
    "    # of all of those attributes.  For example, if a dataset has both 'Race' and 'Gender', the\n",
    "    # combined feature 'Race-Gender' is created that has attributes, e.g., 'White-Woman'.\n",
    "    sensitive_attrs = dataset.get_sensitive_attributes()\n",
    "    if len(sensitive_attrs) > 1:\n",
    "        new_attr_name = '-'.join(sensitive_attrs)\n",
    "        ## TODO: the below may fail for non-string attributes\n",
    "        processed_data = processed_data.assign(temp_name =\n",
    "                             processed_data[sensitive_attrs].apply('-'.join, axis=1))\n",
    "        processed_data = processed_data.rename(columns = {'temp_name' : new_attr_name})\n",
    "        # dataset.append_sensitive_attribute(new_attr_name)\n",
    "        # privileged_joint_vals = '-'.join(dataset.get_privileged_class_names(\"\"))\n",
    "        # dataset.get_privileged_class_names(\"\").append(privileged_joint_vals)\n",
    "\n",
    "    # Create a one-hot encoding of the categorical variables.\n",
    "    processed_numerical = pd.get_dummies(processed_data,\n",
    "                                         columns = dataset.get_categorical_features())\n",
    "\n",
    "    # Create a version of the numerical data for which the sensitive attribute is binary.\n",
    "    sensitive_attrs = dataset.get_sensitive_attributes_with_joint()\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(\"\")\n",
    "    processed_binsensitive = make_sensitive_attrs_binary(\n",
    "        processed_numerical, sensitive_attrs, privileged_vals)\n",
    "\n",
    "    # Create a version of the categorical data for which the sensitive attributes is binary.\n",
    "    processed_categorical_binsensitive = make_sensitive_attrs_binary(\n",
    "        processed_data, sensitive_attrs,\n",
    "        dataset.get_privileged_class_names(\"\")) ## FIXME\n",
    "    # Make the class attribute numerical if it wasn't already (just for the bin_sensitive version).\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    pos_val = dataset.get_positive_class_val(\"\") ## FIXME\n",
    "\n",
    "    processed_binsensitive = make_class_attr_num(processed_binsensitive, class_attr, pos_val)\n",
    "\n",
    "    return { \"original\": processed_data,\n",
    "             \"numerical\": processed_numerical,\n",
    "             \"numerical-binsensitive\": processed_binsensitive,\n",
    "             \"categorical-binsensitive\": processed_categorical_binsensitive }\n",
    "\n",
    "def make_sensitive_attrs_binary(dataframe, sensitive_attrs, privileged_vals):\n",
    "    newframe = dataframe.copy()\n",
    "    for attr, privileged in zip(sensitive_attrs, privileged_vals):\n",
    "        # replace privileged vals with 1\n",
    "        newframe[attr] = newframe[attr].replace({ privileged : 1 })\n",
    "        # replace all other vals with 0\n",
    "        newframe[attr] = newframe[attr].replace(\"[^1]\", 0, regex = True)\n",
    "    return newframe\n",
    "\n",
    "def make_class_attr_num(dataframe, class_attr, positive_val):\n",
    "    # don't change the class attribute unless its a string (pandas type: object)\n",
    "    if (dataframe[class_attr].dtypes == 'object'):\n",
    "        dataframe[class_attr] = dataframe[class_attr].replace({ positive_val : 1 })\n",
    "        dataframe[class_attr] = dataframe[class_attr].replace(\"[^1]\", 0, regex = True)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choix d'un seul jeu de données\n",
    "Le process n'est appliqué qu'au jeu de données `adult income` à titre d'illustration. De toute façon, les données tranformées sont enregistrées sans le dépôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data: 2399 rows removed from dataset adult\n",
      "\n",
      "-------------------\n",
      "Balance statistics:\n",
      "\n",
      "Class:\n",
      "income-per-year\n",
      "<=50K    22654\n",
      ">50K      7508\n",
      "dtype: int64\n",
      "\n",
      "Sensitive Attribute:\n",
      "race\n",
      "Amer-Indian-Eskimo      286\n",
      "Asian-Pac-Islander      895\n",
      "Black                  2817\n",
      "Other                   231\n",
      "White                 25933\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "sex\n",
      "Female     9782\n",
      "Male      20380\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset=DATASETS[1]\n",
    "data_frame = dataset.load_raw_dataset()\n",
    "d = preprocess(dataset, data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, les données transformées sont sauvegardées par défaut dans l'architecture locale du dépôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_original.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_numerical.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_numerical-binsensitive.csv\n",
      "Writing data to: /home-local/pbesse/anaconda3/lib/python3.6/site-packages/fairness/data/preprocessed/adult_categorical-binsensitive.csv\n"
     ]
    }
   ],
   "source": [
    "for k, v in d.items():\n",
    "            write_to_file(dataset.get_filename(k), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Exécution des algorithmes de correction\n",
    "### 3.1 Librairie et fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "\n",
    "from fairness import results\n",
    "from fairness.data.objects.list import DATASETS, get_dataset_names\n",
    "from fairness.data.objects.ProcessedData import ProcessedData\n",
    "from fairness.algorithms.list import ALGORITHMS\n",
    "from fairness.metrics.list import get_metrics\n",
    "\n",
    "from fairness.algorithms.ParamGridSearch import ParamGridSearch\n",
    "\n",
    "NUM_TRIALS_DEFAULT = 10\n",
    "\n",
    "def get_algorithm_names():\n",
    "    result = [algorithm.get_name() for algorithm in ALGORITHMS]\n",
    "    print(\"Available algorithms:\")\n",
    "    for a in result:\n",
    "        print(\"  %s\" % a)\n",
    "    return result\n",
    "\n",
    "def run(num_trials = NUM_TRIALS_DEFAULT, dataset = get_dataset_names(),\n",
    "        algorithm = get_algorithm_names()):\n",
    "    algorithms_to_run = algorithm\n",
    "\n",
    "    print(\"Datasets: '%s'\" % dataset)\n",
    "    for dataset_obj in DATASETS:\n",
    "        if not dataset_obj.get_dataset_name() in dataset:\n",
    "            continue\n",
    "\n",
    "        print(\"\\nEvaluating dataset:\" + dataset_obj.get_dataset_name())\n",
    "\n",
    "        processed_dataset = ProcessedData(dataset_obj)\n",
    "        train_test_splits = processed_dataset.create_train_test_splits(num_trials)\n",
    "\n",
    "        all_sensitive_attributes = dataset_obj.get_sensitive_attributes_with_joint()\n",
    "        for sensitive in all_sensitive_attributes:\n",
    "\n",
    "            print(\"Sensitive attribute:\" + sensitive)\n",
    "\n",
    "            detailed_files = dict((k, create_detailed_file(\n",
    "                                          dataset_obj.get_results_filename(sensitive, k),\n",
    "                                          dataset_obj,\n",
    "                                          processed_dataset.get_sensitive_values(k), k))\n",
    "                for k in train_test_splits.keys())\n",
    "\n",
    "            for algorithm in ALGORITHMS:\n",
    "                if not algorithm.get_name() in algorithms_to_run:\n",
    "                    continue\n",
    "\n",
    "                print(\"    Algorithm: %s\" % algorithm.get_name())\n",
    "                print(\"       supported types: %s\" % algorithm.get_supported_data_types())\n",
    "                if algorithm.__class__ is ParamGridSearch:\n",
    "                    param_files =  \\\n",
    "                        dict((k, create_detailed_file(\n",
    "                                     dataset_obj.get_param_results_filename(sensitive, k,\n",
    "                                                                            algorithm.get_name()),\n",
    "                                     dataset_obj, processed_dataset.get_sensitive_values(k), k))\n",
    "                          for k in train_test_splits.keys())\n",
    "                for i in range(0, num_trials):\n",
    "                    for supported_tag in algorithm.get_supported_data_types():\n",
    "                        train, test = train_test_splits[supported_tag][i]\n",
    "                        try:\n",
    "                            params, results, param_results =  \\\n",
    "                                run_eval_alg(algorithm, train, test, dataset_obj, processed_dataset,\n",
    "                                             all_sensitive_attributes, sensitive, supported_tag)\n",
    "                        except Exception as e:\n",
    "                            import traceback\n",
    "                            traceback.print_exc(file=sys.stderr)\n",
    "                            print(\"Failed: %s\" % e, file=sys.stderr)\n",
    "                        else:\n",
    "                            write_alg_results(detailed_files[supported_tag],\n",
    "                                              algorithm.get_name(), params, i, results)\n",
    "                            if algorithm.__class__ is ParamGridSearch:\n",
    "                                for params, results in param_results:\n",
    "                                    write_alg_results(param_files[supported_tag],\n",
    "                                                      algorithm.get_name(), params, i, results)\n",
    "\n",
    "            print(\"Results written to:\")\n",
    "            for supported_tag in algorithm.get_supported_data_types():\n",
    "                print(\"    %s\" % dataset_obj.get_results_filename(sensitive, supported_tag))\n",
    "\n",
    "            for detailed_file in detailed_files.values():\n",
    "                detailed_file.close()\n",
    "\n",
    "def write_alg_results(file_handle, alg_name, params, run_id, results_list):\n",
    "    line = alg_name + ','\n",
    "    params = \";\".join(\"%s=%s\" % (k, v) for (k, v) in params.items())\n",
    "    line += params + (',%s,' % run_id)\n",
    "    line += ','.join(str(x) for x in results_list) + '\\n'\n",
    "    file_handle.write(line)\n",
    "\n",
    "def run_eval_alg(algorithm, train, test, dataset, processed_data, all_sensitive_attributes,\n",
    "                 single_sensitive, tag):\n",
    "    \"\"\"\n",
    "    Runs the algorithm and gets the resulting metric evaluations.\n",
    "    \"\"\"\n",
    "    privileged_vals = dataset.get_privileged_class_names_with_joint(tag)\n",
    "    positive_val = dataset.get_positive_class_val(tag)\n",
    "\n",
    "    # get the actual classifications and sensitive attributes\n",
    "    actual = test[dataset.get_class_attribute()].values.tolist()\n",
    "    sensitive = test[single_sensitive].values.tolist()\n",
    "\n",
    "    predicted, params, predictions_list =  \\\n",
    "        run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "                privileged_vals, positive_val)\n",
    "\n",
    "    # make dictionary mapping sensitive names to sensitive attr test data lists\n",
    "    dict_sensitive_lists = {}\n",
    "    for sens in all_sensitive_attributes:\n",
    "        dict_sensitive_lists[sens] = test[sens].values.tolist()\n",
    "\n",
    "    sensitive_dict = processed_data.get_sensitive_values(tag)\n",
    "    one_run_results = []\n",
    "    for metric in get_metrics(dataset, sensitive_dict, tag):\n",
    "        result = metric.calc(actual, predicted, dict_sensitive_lists, single_sensitive,\n",
    "                             privileged_vals, positive_val)\n",
    "        one_run_results.append(result)\n",
    "\n",
    "    # handling the set of predictions returned by ParamGridSearch\n",
    "    results_lol = []\n",
    "    if len(predictions_list) > 0:\n",
    "        for param_name, param_val, predictions in predictions_list:\n",
    "            params_dict = { param_name : param_val }\n",
    "            results = []\n",
    "            for metric in get_metrics(dataset, sensitive_dict, tag):\n",
    "                result = metric.calc(actual, predictions, dict_sensitive_lists, single_sensitive,\n",
    "                                     privileged_vals, positive_val)\n",
    "                results.append(result)\n",
    "            results_lol.append( (params_dict, results) )\n",
    "\n",
    "    return params, one_run_results, results_lol\n",
    "\n",
    "def run_alg(algorithm, train, test, dataset, all_sensitive_attributes, single_sensitive,\n",
    "            privileged_vals, positive_val):\n",
    "    class_attr = dataset.get_class_attribute()\n",
    "    params = algorithm.get_default_params()\n",
    "\n",
    "    # Note: the training and test set here still include the sensitive attributes because\n",
    "    # some fairness aware algorithms may need those in the dataset.  They should be removed\n",
    "    # before any model training is done.\n",
    "    predictions, predictions_list =  \\\n",
    "        algorithm.run(train, test, class_attr, positive_val, all_sensitive_attributes,\n",
    "                      single_sensitive, privileged_vals, params)\n",
    "\n",
    "    return predictions, params, predictions_list\n",
    "\n",
    "\n",
    "def get_dict_sensitive_vals(dict_sensitive_lists):\n",
    "    \"\"\"\n",
    "    Takes a dictionary mapping sensitive attributes to lists in the test data and returns a\n",
    "    dictionary mapping sensitive attributes to lists containing each sensitive value only once.\n",
    "    \"\"\"\n",
    "    newdict = {}\n",
    "    for sens in dict_sensitive_lists:\n",
    "         sensitive = dict_sensitive_lists[sens]\n",
    "         newdict[sens] = list(set(sensitive))\n",
    "    return newdict\n",
    "\n",
    "def create_detailed_file(filename, dataset, sensitive_dict, tag):\n",
    "    return results.ResultsFile(filename, dataset, sensitive_dict, tag)\n",
    "    # f = open(filename, 'w')\n",
    "    # f.write(get_detailed_metrics_header(dataset, sensitive_dict, tag) + '\\n')\n",
    "    # return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  Liste des algorithmes disponibles\n",
    "Il ya ceux classiques d'apprentissage sans correction de biais (base line) et ceux opérant par loyauté par une transformation des données, ou en contraignant l'étape d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available algorithms:\n",
      "  SVM\n",
      "  GaussianNB\n",
      "  LR\n",
      "  DecisionTree\n",
      "  Kamishima\n",
      "  Calders\n",
      "  ZafarBaseline\n",
      "  ZafarFairness\n",
      "  ZafarAccuracy\n",
      "  Kamishima-accuracy\n",
      "  Kamishima-DIavgall\n",
      "  Feldman-SVM\n",
      "  Feldman-GaussianNB\n",
      "  Feldman-LR\n",
      "  Feldman-DecisionTree\n",
      "  Feldman-SVM-DIavgall\n",
      "  Feldman-SVM-accuracy\n",
      "  Feldman-GaussianNB-DIavgall\n",
      "  Feldman-GaussianNB-accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Calders'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_algorithm_names()[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Code d'exécution pour un algorithme\n",
    "`num_trials` exécutions pour un jeu de données et un algorithme déterminés. Les résultats sont empilés dans un ensemble de fichiers. \n",
    "\n",
    "**Attention** Certains algorithmes (Calders) plantent alors que d'autres (Feldman-SVM-DIavgall) sont excessivement longs. La cellule ci-dessous  a été exécutée pour différents algorihtmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: 'adult'\n",
      "\n",
      "Evaluating dataset:adult\n",
      "Sensitive attribute:race\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_race_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_race_numerical.csv\n",
      "Sensitive attribute:sex\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_sex_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_sex_numerical.csv\n",
      "Sensitive attribute:race-sex\n",
      "    Algorithm: DecisionTree\n",
      "       supported types: {'numerical-binsensitive', 'numerical'}\n",
      "Results written to:\n",
      "    /home-local/pbesse/.fairness/results/adult_race-sex_numerical-binsensitive.csv\n",
      "    /home-local/pbesse/.fairness/results/adult_race-sex_numerical.csv\n"
     ]
    }
   ],
   "source": [
    "run(num_trials = 2, dataset = 'adult',algorithm = 'DecisionTree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Résultats\n",
    "Tous les résultats intermédiaires sont stockés dans des fichiers archivés dans un répertoire masqué. Ces fichiers sont destinés à être lus par le programme `results.py` capable d'en extraire les valeurs utiles au calcul d'une liste très exhaustive de métriques. Ces derniers résultats sont également stockés puis l'exécution du programme `analysis.py` fournit les graphiques de l'article à l'aide de fonctions R (`ggplot`). \n",
    "\n",
    "Il s'agit donc, dans ce calepin, d'extraire de ces fichiers à titre d'exemple les indicateurs plus pertinents sans recherche d'exhaustivité. *Soucis*: les intitulés ne sont pas très explicites et le nombre de modalités de la variable d'origine ethnique fait exploser la combinatoire de résultats possibles. Il semble important de se limiter à la seule prise en compte de variables sensibles binaires: genre ou origine ethnique caucasien *vs.* non caucasien. La prise en compte des interactions entre les deux variables sensibles introduit également une forte complexité pas indispensable en première lecture.\n",
    "\n",
    "Voici trois colonnes extraites des fichiers: `adult_sex_numerical-binsensitive.csv` et `adult_race_numerical-binsensitive.csv` qui en comporte 2 * 127 lorsque les variables sensibles sont considérées binaires. Dans le cas contraire, si toutes les modalités d'origine sont prises en compte, le fichier `adult_sex_numerical.csv` comporte 389 colonnes!\n",
    "\n",
    "\n",
    "\n",
    "|Algorithme | Paramètre | DI - genre| DI - origine|\n",
    "|----------|--------|-------|----|\n",
    "|SVM| none | 0.25|0.62|\n",
    "|SVM| none | 0.28|0.64|\n",
    "|GaussianNB| none | 0.32|0.62|\n",
    "|GaussianNB| none | 0.34|0.60|\n",
    "|LR | none | 0.33 |0.61|\n",
    "|LR | none | 0.30|0.61|\n",
    "|DecisionTree|none|0.44|0.70|\n",
    "|DecisionTree|none|0.43|0.60|\n",
    "|Kamishima|eta=1.0| 0.28|0.53|\n",
    "|Kamishima|eta=1.0|0.30|0.58|\n",
    "|ZafarFairness|\tc=0.001\t| 0.29|0.50|\n",
    "|ZafarFairness|\tc=0.001\t| 0.29|0.53|\n",
    "|Kamishima-DIavgall|eta=1.0| 0.28|0.53|\n",
    "|Kamishima-DIavgall|eta=1.0|0.30|0.58|\n",
    "|Feldman-SVM|lambda=1.0|0.30|0.68|\n",
    "|Feldman-SVM|lambda=1.0|0.37|0.62|\n",
    "|Feldman-SVM-DIavgall|lambda=0.35|0.30|0.81|\n",
    "|Feldman-SVM-DIavgall|lambda=0.45|0.35|0.68|\n",
    "\n",
    "**Remarques**\n",
    "- Compte tenu que seulement 2 exécutions ont été réalisées, les résultats semblent cohérents avec les graphiques de l'article (figure 6 page 16).\n",
    "- En revanche, les *DI* obtenus pour les algorithmes d'apprentissage seuls ne sont pas cohérents avec ceux calculés en R. \n",
    "- Les corrections de biais vis-à-vis du genre ne sont pas très probantes pour les couples algo x paramètre exécutés par rapports aux algorithmes seuls qui constituent des *base lines*. C'est explicable vis-à-vis de la variable genre car l'optimisation des paramètres a sans doute été réalisée avec pour objectif de réduire le biais liée à la variable origine. Néanmoins même dans ce cas, cette réduction n'est pas exceptionnelle.\n",
    "- Il faudrait aussi ajouter la précision et choisir un indicateur de taux d'erreur conditionnel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Vérification \"manuelle\" des résultats\n",
    "Comme pour l'exécution du code R, les différents indicateurs (DI, précision) sont estimés pour deux algorithmes d'apprentissage: régression logistique et random forest. L'objectif est de traquer les différences observées entre les résultats obtenus avec R et Python.\n",
    "\n",
    "### 5.1 Sélection du jeu de données adult prétraité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairness.data.objects.ProcessedData.ProcessedData at 0x7fe3d0fc9438>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProcessedData(DATASETS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = ProcessedData(DATASETS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splits = processed_dataset.create_train_test_splits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['original', 'numerical', 'numerical-binsensitive', 'categorical-binsensitive'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_splits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres de l'algorithme de régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numerical', 'numerical-binsensitive'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALGORITHMS[2].get_supported_data_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echantillons d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_splits['numerical-binsensitive'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'education-num', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
       "       'hours-per-week', 'income-per-year', 'race-sex',\n",
       "       'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Private',\n",
       "       'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc',\n",
       "       'workclass_State-gov', 'workclass_Without-pay', 'education_10th',\n",
       "       'education_11th', 'education_12th', 'education_1st-4th',\n",
       "       'education_5th-6th', 'education_7th-8th', 'education_9th',\n",
       "       'education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors',\n",
       "       'education_Doctorate', 'education_HS-grad', 'education_Masters',\n",
       "       'education_Preschool', 'education_Prof-school',\n",
       "       'education_Some-college', 'marital-status_Divorced',\n",
       "       'marital-status_Married-AF-spouse', 'marital-status_Married-civ-spouse',\n",
       "       'marital-status_Married-spouse-absent', 'marital-status_Never-married',\n",
       "       'marital-status_Separated', 'marital-status_Widowed',\n",
       "       'occupation_Adm-clerical', 'occupation_Armed-Forces',\n",
       "       'occupation_Craft-repair', 'occupation_Exec-managerial',\n",
       "       'occupation_Farming-fishing', 'occupation_Handlers-cleaners',\n",
       "       'occupation_Machine-op-inspct', 'occupation_Other-service',\n",
       "       'occupation_Priv-house-serv', 'occupation_Prof-specialty',\n",
       "       'occupation_Protective-serv', 'occupation_Sales',\n",
       "       'occupation_Tech-support', 'occupation_Transport-moving',\n",
       "       'relationship_Husband', 'relationship_Not-in-family',\n",
       "       'relationship_Other-relative', 'relationship_Own-child',\n",
       "       'relationship_Unmarried', 'relationship_Wife',\n",
       "       'native-country_Cambodia', 'native-country_Canada',\n",
       "       'native-country_China', 'native-country_Columbia',\n",
       "       'native-country_Cuba', 'native-country_Dominican-Republic',\n",
       "       'native-country_Ecuador', 'native-country_El-Salvador',\n",
       "       'native-country_England', 'native-country_France',\n",
       "       'native-country_Germany', 'native-country_Greece',\n",
       "       'native-country_Guatemala', 'native-country_Haiti',\n",
       "       'native-country_Holand-Netherlands', 'native-country_Honduras',\n",
       "       'native-country_Hong', 'native-country_Hungary', 'native-country_India',\n",
       "       'native-country_Iran', 'native-country_Ireland', 'native-country_Italy',\n",
       "       'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos',\n",
       "       'native-country_Mexico', 'native-country_Nicaragua',\n",
       "       'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru',\n",
       "       'native-country_Philippines', 'native-country_Poland',\n",
       "       'native-country_Portugal', 'native-country_Puerto-Rico',\n",
       "       'native-country_Scotland', 'native-country_South',\n",
       "       'native-country_Taiwan', 'native-country_Thailand',\n",
       "       'native-country_Trinadad&Tobago', 'native-country_United-States',\n",
       "       'native-country_Vietnam', 'native-country_Yugoslavia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2  Estimation ponctuelle du *DI*  \n",
    "$DI=\\frac{n_{21}}{(n_{11}+n_{21})}/\\frac{n_{22}}{(n_{12}+n_{22})}.$\n",
    "#### Données de base\n",
    "\n",
    "*DI Apprentissage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 0     1\n",
      "income-per-year            \n",
      "0                5832  9223\n",
      "1                 740  4313\n"
     ]
    }
   ],
   "source": [
    "table=pd.crosstab(train[\"income-per-year\"],train[\"sex\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35338251113881103"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(740/(5832+740))/(4313/(4313+9223))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DI Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 0     1\n",
      "income-per-year            \n",
      "0                2838  4761\n",
      "1                 372  2083\n"
     ]
    }
   ],
   "source": [
    "table2=pd.crosstab(test[\"income-per-year\"],test[\"sex\"])\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3807664179539755"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(372/(372+2838))/(2083/(2083+4761))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les *DI* correspondent bien à ceux estimés dans R.\n",
    "\n",
    "### 5.3 Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=train[\"income-per-year\"]\n",
    "Y_test=test[\"income-per-year\"]\n",
    "X_train=train\n",
    "del X_train[\"income-per-year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test\n",
    "del X_test[\"income-per-year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation et optimisation Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home-local/pbesse/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur score = 0.844639, Meilleur paramètre = {'C': 1.5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "param=[{\"C\":[1,1.2,1.5,1.7,2,3,4]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"), param,cv=5,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui même un estimateur\n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prévision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = logitOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(Y_hat,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income-per-year     0     1\n",
      "row_0                      \n",
      "0                7084   980\n",
      "1                 515  1475\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513029639944301"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(7084+1475)/(7084+1475+515+980)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est plutôt meilleure d'un point que celle trouvée en R. C'est probablement dû à la complexité du modèle incluant beaucoup plus de variables / modalités et l'interaction `race-sex`.\n",
    "\n",
    "#### *DI* de la prévision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex       0     1\n",
      "row_0            \n",
      "0      2970  5094\n",
      "1       240  1750\n"
     ]
    }
   ],
   "source": [
    "table=pd.crosstab(Y_hat,X_test[\"sex\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29240053404539385"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(240/(240+2970))/(1750/(1750+5094))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Random forest\n",
    "#### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None, bootstrap=True)\n",
    "# apprentissage\n",
    "rfFit = forest.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prévision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income-per-year     0     1\n",
      "row_0                      \n",
      "0                6959   930\n",
      "1                 640  1525\n"
     ]
    }
   ],
   "source": [
    "# prévision\n",
    "Y_hrf = rfFit.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(Y_hrf,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8422586154928162"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6858+1525)/(6858+1525+640+930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *DI* de la prévision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex       0     1\n",
      "row_0            \n",
      "0      2947  4942\n",
      "1       263  1902\n"
     ]
    }
   ],
   "source": [
    "table=pd.crosstab(Y_hrf,X_test[\"sex\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29481542629335905"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(263/(263+2947))/(1902/(1902+4942))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Discussion\n",
    "Les résultats des exécutions précédentes sont conformes à ceux de l'artilce mais évidemment tout n'a pas été vérifié; notamment pour les autres jeux de données et à cause du plantage de certains algos. \n",
    "\n",
    "En résumé: \n",
    "- Précision:\n",
    "   - Conformément à l'article, la régression logistique fait mieux que celle exécutée en R qui est plus frustre: moins de variables prises en compte et pas l'interaction `sex-race`.\n",
    "   - Random forest fournit des résultats très voisins de ceux de la régression logistique mais en retrait par rapport à ceux de R... il faudrait itérer les séparations apprentissage / test  pour confirmer et contrôler la cohérence du choix de `max_features` *vs.* `mtry`.\n",
    "- *DI*\n",
    "   - le *DI* de la régression logistique est conforme à l'article et donc aux exécutions précédentes, plus élevé que celui de R.\n",
    "   - celui de *Random Forest* est en revanche plus faible que celui de R !\n",
    "- Il faudrait:\n",
    "   - compléter les résultats en itérant ceux-ci-dessus\n",
    "   - comprendre mieux pourquoi la régression logistique fait mieux en python au contraire de random forest. \n",
    "   - ajouter d'autres modèles: arbre binaire de décision, SVM, classifieur bayésien...\n",
    "\n",
    "Les différences observées entre R et Python proviennent donc bien des pré-traitements très différents et des listes des variables finalement retenues.\n",
    "   \n",
    "### Suggestion de compléments\n",
    "#### Compléter \n",
    "les comparaisons en repartant des données initiales en python et contrôlant le prétraitement et les variables incluses dans les modèles.\n",
    "- estimer le DI\n",
    "- régression logistique, arbre, random forest, autre ?\n",
    "- estimer les différents DI\n",
    "\n",
    "#### Correction élémentaire du biais\n",
    "Le problème principal réside dans le choix du seuil plus favorable pour les femmes: \n",
    "- partager l'échantillon en trois parties train, validation et test ou voir ce qui est proposé dans Calders et Verwer (2010)\n",
    "- estimer le modèle (train)\n",
    "- chercher un bon seuil (validation) pour un DI de 0.38 des données initiales\n",
    "- estimer le DI sur le test pour vérifier s'il est correct ou pas.\n",
    "\n",
    "Cette procédure est bien moins élégante que celles des algoritmes cherchant une indépendance entre la décision et la variable sensible mais tellement plus simple et rapide à mettre en place...\n",
    "\n",
    "### Ajouter d'autres algorithmes ?\n",
    "Qu'en est il d'autres approches de débiaisage: \n",
    "- par transport optimal\n",
    "    - [Article](https://arxiv.org/pdf/1806.03195.pdf)\n",
    "    - [Code en R](https://github.com/JMLToulouse/FairLearning)\n",
    "- par GAN ou DANN?\n",
    "    - [Article](http://jmlr.org/papers/volume17/15-239/15-239.pdf)\n",
    "    - [Code Keras](https://github.com/michetonu/gradient_reversal_keras_tf)\n",
    "- Un gros travail pour comparer avec le site [aif360.mybluemix.net/](https://aif360.mybluemix.net/)!\n",
    "\n",
    "Une fois ces tâches de comparaison réalisées et les bons outils sélectionnés *consensuellement*... il serait pertinent de produire une librairie efficace car parallélisée et industrialisable de correction du biais pour aborder des problèmes de la vraie vie et pas seulement des jeux de données publics mais élémentaires même si volumineux. Comme le montntre ces différentes expérimentations, le contexte, les données disponibles, leur qualité, les questions de confidentialité... sont ou seront prépondérantes sur les méthodes ou algoritmes à mettre en oeuvre dans le cadre d'un audit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "Calders T., Verwer S. (2010). Three naive Bayes approaches for discrimination-free classification, Data Mining and Knowledge Discovery, 21 (2), pp 277–292.\n",
    "\n",
    "Friedler S., Scheidegger C., Venkatasubramanian S., Choudhary S., Hamilton E., Roth D. (2019). [A comparative study of fairness-enhancing interventions in machine learning](https://dl.acm.org/citation.cfm?id=3287589), Proceedings of the Conference on Fairness, Accountability, and Transparency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
