{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"height:80px; display: inline\"  alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel en python: Détecter, Mesurer, la Discrimination Indirecte des Algorithmes d'Apprentissage Statistique\n",
    "\n",
    "\n",
    "**Résumé**\n",
    "L'objectif de ce calepin est de proposer les codes en python permettant de reproduire une partie des résultats obtenus avec le [tutoriel en R](https://github.com/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-R-biasDetectionCourt.ipynb) de détection et correction élémentaire des biais et discriminations. Il est  indispensable d'avoir exécuté et étudié le [tutoriel en R](https://github.com/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-R-biasDetectionCourt.ipynb) très explicite pour la compréhension de la démarche avant d'exécuter celui-ci en Python pour s'approprier les codes nécessaires.\n",
    "\n",
    "**Remarques** \n",
    "- Tout le début: préparation des données et apprentissage des algorithmes reprend (copier coller) les codes du [tutoriel](https://github.com/wikistat/Apprentissage/blob/master/Adult-Census/Apprent-Python-AdultCensus.ipynb) du dépôt sur l'[apprentissage](https://github.com/wikistat/Apprentissage).\n",
    "- La production des codes de ce calepin a largement bénéficié de la contribution des étudiants de l'INSA de Toulouse, spécialité *Mathématiques Appliquées* promotion 2020, notamment celle de Paul Charnay et Emmeline Monnédières.\n",
    "- Ce tutoriel peut être exécuté en local après chargement ou clône du dépôt ou encore dans le nuage *Google Colab* en cliquant sur le lien ci-dessous:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-Python-biasDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 Les données\n",
    "Des données publiques disponibles sur le site [UCI repository](http://archive.ics.uci.edu/ml/) sont extraites de la base de données issue du recensement réalisé aux Etats Unis en 1994. Les deux fichiers *train* et *test* on été réunis en un seul. Ces données son largement utilisées et font référence comme outil de *benchmark* pour comparer les performances de méthodes d’apprentissage. L’objectif est de prévoir, avec plus ou moins de biais, la variable binaire \"revenu annuel\" supérieur ou inférieur à 50k$. Cette prévision n'impacte pas la personne mais comme la démarche et le contexte sont tout à fait similaires à ce que pourrait faire une banque pour évaluer un risque de crédit, c'est exemple est très illustratif. Ce jeu de données est systématiquement utilisé (bac à sable) pour évaluer les propriétés des algorithmes d'apprentissage équitable ou loyal (*fair learning*) car, contrairement à beaucoup d'autres jeux de données utilisés pour cette fin (*e.g. german credit bank*), la vraie valeur de la variable cible est connue ainsi que l'origine ethnique des personnes concernées.\n",
    "\n",
    "Dans les données initiales, 48.842 individus sont décrits par les 14 variables du tableau ci-dessous:\n",
    "\n",
    "\n",
    "|Num|Libellé|Ensemble de valeurs|\n",
    "|-|---------:|-------------------:|\n",
    "|1|`Age`|real|\n",
    "|2|`workClass`|Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked|\n",
    "|3|`fnlwgt`|real|\n",
    "|4|`education`|Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool|\n",
    "|5|`educNum`|integer|\n",
    "|6|`mariStat`|Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse|\n",
    "|7|`occup`|Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces|\n",
    "|8|`relationship`|Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "|9|`origEthn`|White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black|\n",
    "|10|`sex`|Female, Male|\n",
    "|11|`capitalGain`|real| \n",
    "|12|`capitalLoss`|real|\n",
    "|13|`hoursWeek`|real|\n",
    "|14|`nativCountry`|United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands|\n",
    "|15|`income`|>50K, <=50K|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Préparation des données\n",
    "\n",
    "Le traitement commence par une préparation détaillée des données:\n",
    "- lecture et prise en charge des données en remarquant que la variable `fnlwgt` (Final sampling weight) a un [statut particulier](http://web.cs.wpi.edu/~cs4341/C00/Projects/fnlwgt) pas très clair; elle est éliminée;\n",
    "- suppression des observations présentant des données manquantes, erreurs ou incohérences,\n",
    "- regroupement des modalités trop rares, \n",
    "- suppression des variables redondantes. \n",
    "\n",
    "Cette phase est notoirement différente de la stratégie de Friedler et al. (2019) qui, par principe, analysent automatiquement toutes les données brutes sans le préalable d'une analyse statistique descriptive élémentaire.\n",
    "\n",
    "### 1.3 Estimation des biais\n",
    "\n",
    "Parmi tous les critères de biais existant susceptibles de mettre en évidence une discrimination indirecte (Zliobaitė, 2015)), trois sont privilégiés (cf. Vermat et Rubin, 2018): \n",
    "1. discrimination indirecte par l'effet disproportionné: *disparate impact* ou *demographic equality*\n",
    "2. comparaison des taux d'erreur conditionnels: *overall error equality*\n",
    "3. comparaison des rapports de cote: *Conditional procedure accuracy equality* ou *disparate mistreatment* ou *equalized odds*.\n",
    "\n",
    "L'effet disproportionné est considéré ici en priorité mais les outils  (fonctions) d'estimation par intervalle de confiance des autres biais sont fournis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Pris en charge et exploration des données\n",
    "Lors de cette phase de travail, il y a deux points de vue radicalement différents. \n",
    "- Celui illustré par Friedler et al. (2019) consiste à entraîner un algorithme sur les données brutes sans exploration \"humaine\" préalable faisant appel à des compétences statistiques; par principe, tout est automatisé.\n",
    "- Celui proposé dans ce tutoriel est le résultat d'une démarche nécessitant des compétences élémentaires  statistiques pour explorer les données, comprendre leur structure, détecter les problèmes potentiels: données manquantes, atypiques, biais, classes rares, distributions \"anormales\"...) afin d'y remédier au mieux de l'intérêt de l'objectif poursuivi.\n",
    "\n",
    "Remarquons que ce deuxièmme point de vue de connaissance des données semble plus respectueux des [lignes directrices des experts de la CE pour une IA igne de confiance](https://ec.europa.eu/futurium/en/ai-alliance-consultation) et anticipe donc un futur réglement européen annoncé par le [livre blanc](https://ec.europa.eu/info/sites/info/files/commission-white-paper-artificial-intelligence-feb2020_fr.pdf).\n",
    "### 2.1 Lecture \n",
    "Deux possibilités pour charger les données issues du dépôt de l'UCI en fonction du mode d'exécution adopté; en local après avoir installé Python et ses librairies ou à distance dans le nuage *Google Colab*. \n",
    "1. Dans le premier cas, les données sont chargées en même temps que le dépôt *Github*, **changer le chemin d'accès en conséquence** en décommentant la commande `path` ci-dessous afin d'économiser le temps de chargement. \n",
    "2. Dans le deuxième cas, <a href=\"https://colab.research.google.com/github/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-Python-biasDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "les données sont chargées et lues dans l'environnement de *Google Colab*; il est nécessaire d'exécuter la cellule suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter cette cellule lors d'une exécution dans le nuage Google Colab\n",
    "# Sinon, les données sont déjà chargées\n",
    "! wget -P . https://github.com/wikistat/Fair-ML-4-Ethical-AI/raw/master/AdultCensus/adultTrainTest.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.420370Z",
     "start_time": "2020-03-15T07:21:51.845027Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "adult=pd.read_csv('adultTrainTest.csv')\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regroupement des modalités trop peu fréquentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.427229Z",
     "start_time": "2020-03-15T07:21:52.422816Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_categorical_data(df, column_name):\n",
    "    cat_columns = pd.Categorical(df[column_name], ordered=False)\n",
    "    return cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.441096Z",
     "start_time": "2020-03-15T07:21:52.430168Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"mariStat\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.457357Z",
     "start_time": "2020-03-15T07:21:52.443414Z"
    }
   },
   "outputs": [],
   "source": [
    "# mariStat\n",
    "cat_name_dic = {\" Never-married\": \"Never-Married\", \" Married-AF-spouse\": \"Married\",\n",
    "                \" Married-civ-spouse\": \"Married\", \" Married-spouse-absent\": \"Not-Married\",\n",
    "                \" Separated\": \"Not-Married\", \" Divorced\": \"Not-Married\", \" Widowed\": \"Widowed\"}\n",
    "adult['mariStat'] = adult.mariStat.map(cat_name_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.466956Z",
     "start_time": "2020-03-15T07:21:52.459275Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"nativCountry\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.491965Z",
     "start_time": "2020-03-15T07:21:52.468699Z"
    }
   },
   "outputs": [],
   "source": [
    "# nativCountry\n",
    "cat_country = {\" Cambodia\": \"SE-Asia\", \" Canada\": \"British-Commonwealth\", \" China\": \"China\", \" Columbia\": \"South-America\",\n",
    "               \" Cuba\": \"Other\", \" Dominican-Republic\": \"Latin-America\", \" Ecuador\": \"South-America\",\n",
    "               \" El-Salvador\": \"South-America\", \" England\": \"British-Commonwealth\", \" France\": \"Euro_1\",\n",
    "               \" Germany\": \"Euro_1\", \" Greece\": \"Euro_2\", \" Guatemala\": \"Latin-America\", \" Haiti\": \"Latin-America\",\n",
    "               \" Holand-Netherlands\": \"Euro_1\", \" Honduras\": \"Latin-America\", \" Hong\": \"China\", \" Hungary\": \"Euro_2\",\n",
    "               \" India\": \"British-Commonwealth\", \" Iran\": \"Other\", \" Ireland\": \"British-Commonwealth\", \" Italy\": \"Euro_1\",\n",
    "               \" Jamaica\": \"Latin-America\", \" Japan\": \"Other\", \" Laos\": \"SE-Asia\", \" Mexico\": \"Latin-America\",\n",
    "               \" Nicaragua\": \"Latin-America\", \" Outlying-US(Guam-USVI-etc)\": \"Latin-America\", \" Peru\": \"South-America\",\n",
    "               \" Philippines\": \"SE-Asia\", \" Poland\": \"Euro_2\", \" Portugal\": \"Euro_2\", \" Puerto-Rico\": \"Latin-America\",\n",
    "               \" Scotland\": \"British-Commonwealth\", \" South\": \"Euro_2\", \" Taiwan\": \"China\", \" Thailand\": \"SE-Asia\",\n",
    "               \" Trinadad&Tobago\": \"Latin-America\", \" Vietnam\": \"SE-Asia\", \" United-States\": \"United-States\",\n",
    "               \" Yugoslavia\": \"Euro_2\"}\n",
    "adult[\"nativCountry\"] = adult.nativCountry.map(cat_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.504730Z",
     "start_time": "2020-03-15T07:21:52.494379Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"education\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.521822Z",
     "start_time": "2020-03-15T07:21:52.507850Z"
    }
   },
   "outputs": [],
   "source": [
    "# education\n",
    "cat_educ = {\" 10th\": \"Dropout\", \" 11th\": \"Dropout\", \" 12th\": \"Dropout\", \" 1st-4th\": \"Dropout\", \" 5th-6th\": \"Dropout\",\n",
    "            \" 7th-8th\": \"Dropout\", \" 9th\": \"Dropout\", \" Assoc-acdm\": \"Associates\", \" Assoc-voc\": \"Associates\",\n",
    "            \" Bachelors\": \"Bachelors\", \" Doctorate\": \"Doctorate\", \" HS-grad\": \"HS-grad\", \" Masters\": \"Masters\",\n",
    "            \" Preschool\": \"Dropout\", \" Prof-school\": \"Prof-School\", \" Some-college\": \"HS-Graduate\"}\n",
    "adult[\"education\"] = adult.education.map(cat_educ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.536215Z",
     "start_time": "2020-03-15T07:21:52.524801Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"workClass\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.559389Z",
     "start_time": "2020-03-15T07:21:52.538679Z"
    }
   },
   "outputs": [],
   "source": [
    "# workClass\n",
    "cat_work = {\" Federal-gov\": \"Federal-Govt\", \" Local-gov\": \"Other-Govt\", \" State-gov\": \"Other-Govt\", \" Private\": \"Private\",\n",
    "            \" Self-emp-inc\": \"Self-Employed\", \" Self-emp-not-inc\": \"Self-Employed\", \" Without-pay\": \"Not-Working\",\n",
    "            \" Never-worked\": \"Not-Working\"}\n",
    "adult[\"workClass\"] = adult.workClass.map(cat_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.575604Z",
     "start_time": "2020-03-15T07:21:52.561709Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"occup\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.593468Z",
     "start_time": "2020-03-15T07:21:52.577916Z"
    }
   },
   "outputs": [],
   "source": [
    "# occup\n",
    "cat_occup = {\" Adm-clerical\": \"Admin\", \" Craft-repair\": \"Blue-Collar\", \" Exec-managerial\": \"White-Collar\",\n",
    "             \" Farming-fishing\": \"Blue-Collar\", \" Handlers-cleaners\": \"Blue-Collar\", \" Machine-op-inspct\": \"Blue-Collar\",\n",
    "             \" Other-service\": \"Service\", \" Priv-house-serv\": \"Service\", \" Prof-specialty\": \"Professional\",\n",
    "             \" Protective-serv\": \"Other-occups\", \" Sales\": \"Sales\", \" Tech-support\": \"Other-occups\",\n",
    "             \" Transport-moving\": \"Blue-Collar\"}\n",
    "adult[\"occup\"] = adult.occup.map(cat_occup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.609086Z",
     "start_time": "2020-03-15T07:21:52.595392Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"origEthn\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.624534Z",
     "start_time": "2020-03-15T07:21:52.611595Z"
    }
   },
   "outputs": [],
   "source": [
    "# origEthn\n",
    "cat_orig = {\" White\": \"CaucYes\", \" Black\": \"CaucNo\", \" Amer-Indian-Eskimo\": \"CaucNo\", \" Asian-Pac-Islander\": \"CaucNo\",\n",
    "            \" Other\": \"CaucNo\"}\n",
    "adult[\"origEthn\"] = adult.origEthn.map(cat_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.637060Z",
     "start_time": "2020-03-15T07:21:52.626654Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"income\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.646057Z",
     "start_time": "2020-03-15T07:21:52.639238Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_categorical_data_rename(df, column_name, cat_name_dic):\n",
    "    cat_columns = pd.Categorical(df[column_name], ordered=False)\n",
    "    new_categorie = [cat_name_dic[old_name] for old_name in cat_columns.categories]\n",
    "    return cat_columns.rename_categories(new_categorie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.660305Z",
     "start_time": "2020-03-15T07:21:52.648095Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"income\"] = create_categorical_data_rename(adult, \"income\", {\" <=50K\": \"incLow\", \" >50K\": \"incHigh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.708650Z",
     "start_time": "2020-03-15T07:21:52.662253Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in [\"workClass\", \"education\", \"mariStat\", \"occup\", \"relationship\", \"origEthn\", \"sex\", \"nativCountry\"]:\n",
    "    adult[name] = create_categorical_data(adult, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transformations de variables quantitatives\n",
    "Les distributions sont trop disymétriques pour intégrer un modèle linéaire sensible à des valeurs extrêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.940161Z",
     "start_time": "2020-03-15T07:21:52.710395Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"capitalGain\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:52.963102Z",
     "start_time": "2020-03-15T07:21:52.942543Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"LcapitalGain\"] = np.log(1 + adult[\"capitalGain\"])\n",
    "adult[\"LcapitalLoss\"] = np.log(1 + adult[\"capitalLoss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Suppressions de données manquantes ou incohérentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.014553Z",
     "start_time": "2020-03-15T07:21:52.965502Z"
    }
   },
   "outputs": [],
   "source": [
    "adult = adult[np.logical_not(adult.isnull().any(axis=1))]\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.040217Z",
     "start_time": "2020-03-15T07:21:53.017035Z"
    }
   },
   "outputs": [],
   "source": [
    "adult = adult[(adult[\"sex\"] != \"Female\") | (adult[\"relationship\"] != \"Husband\")]\n",
    "adult = adult[(adult[\"sex\"] != \"Male\") | (adult[\"relationship\"] != \"Wife\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Description statistique élémentaire\n",
    "Mettre en évidence des difficultés présentes sur certaines variables ou couples de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.107296Z",
     "start_time": "2020-03-15T07:21:53.043096Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la distribution de la variable `age`, de celle `income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.307252Z",
     "start_time": "2020-03-15T07:21:53.110235Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"age\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.323184Z",
     "start_time": "2020-03-15T07:21:53.309265Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.356187Z",
     "start_time": "2020-03-15T07:21:53.329367Z"
    }
   },
   "outputs": [],
   "source": [
    "adult[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:53.840369Z",
     "start_time": "2020-03-15T07:21:53.359430Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.plot(kind=\"scatter\",x=\"age\",y=\"educNum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire des liaisons : `age x hoursWeek`, `age x income`, `sex x income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:54.306443Z",
     "start_time": "2020-03-15T07:21:53.843537Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.plot(kind=\"scatter\",x=\"hoursWeek\",y=\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:54.560423Z",
     "start_time": "2020-03-15T07:21:54.308169Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.boxplot(column=\"age\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire des redondances ci-dessous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:54.600679Z",
     "start_time": "2020-03-15T07:21:54.562792Z"
    }
   },
   "outputs": [],
   "source": [
    "table=pd.crosstab(adult[\"education\"],adult[\"educNum\"])\n",
    "print(table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:54.653812Z",
     "start_time": "2020-03-15T07:21:54.603596Z"
    }
   },
   "outputs": [],
   "source": [
    "table=pd.crosstab(adult[\"mariStat\"],adult[\"relationship\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:54.865264Z",
     "start_time": "2020-03-15T07:21:54.655993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mosaic plots\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(adult,[\"income\",\"sex\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.032721Z",
     "start_time": "2020-03-15T07:21:54.867909Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(adult,[\"income\",\"origEthn\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques modifications comlémentaires sont apportées de la base. Des variables sont supprimées afin de ne garder qu'une seule présence d'une information sensible: genre et origine ethnique.\n",
    "- Suppression de la variable `fnlwgt` qui n'a guère de signification pour cette analyse.\n",
    "- Suppression de la variable `workClass` redondante acvec l'occupation.\n",
    "- Suppression de la variable `education` redondante avec le niveau de diplôme.\n",
    "- Suppression de la variable `capitalGain` et `capitalLoss` remplacée par leur transformations log.\n",
    "- Création d'une variable binaire `Child`: présence ou non d'enfants.\n",
    "- Suppression de la variable `relationship` redondante avec le genre et le statut marital.\n",
    "- Suppression de la variable `nativCountry` redondante avec l'origine ethnique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.039230Z",
     "start_time": "2020-03-15T07:21:55.034175Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"relationship\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.052913Z",
     "start_time": "2020-03-15T07:21:55.043096Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_orig = {' Husband':\"ChildNo\",' Not-in-family':\"ChildNo\",' Other-relative':\"ChildNo\",' Own-child':\"ChildYes\",' Unmarried':\"ChildNo\",' Wife':\"ChildNo\"}\n",
    "adult[\"child\"] = adult.relationship.map(cat_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.070903Z",
     "start_time": "2020-03-15T07:21:55.060493Z"
    }
   },
   "outputs": [],
   "source": [
    "adult=adult.drop([\"workClass\",\"education\",\"fnlwgt\",\"nativCountry\",\"relationship\", \n",
    "                  \"capitalGain\", \"capitalLoss\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.098504Z",
     "start_time": "2020-03-15T07:21:55.073722Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Variables indicatrices\n",
    "\n",
    "**Q** Pourquoi l’introduction de *dummy variables*? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.134795Z",
     "start_time": "2020-03-15T07:21:55.100458Z"
    }
   },
   "outputs": [],
   "source": [
    "adultDum=pd.get_dummies(adult[[\"mariStat\",\"occup\",\"origEthn\",\"sex\",\"child\"]])\n",
    "adultDum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.145935Z",
     "start_time": "2020-03-15T07:21:55.136423Z"
    }
   },
   "outputs": [],
   "source": [
    "adultJoin = adult[[\"age\",\"educNum\",\"hoursWeek\",\"LcapitalGain\",\"LcapitalLoss\",\"income\"]].join(adultDum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.169385Z",
     "start_time": "2020-03-15T07:21:55.147890Z"
    }
   },
   "outputs": [],
   "source": [
    "adultJoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.177568Z",
     "start_time": "2020-03-15T07:21:55.171824Z"
    }
   },
   "outputs": [],
   "source": [
    "adult.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Préparation des échantillons\n",
    "La base de données est réduite par tirage aléatoire puis divisée en deux échantillons d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.195233Z",
     "start_time": "2020-03-15T07:21:55.181135Z"
    }
   },
   "outputs": [],
   "source": [
    "# possibilité de réduire la taille de l'apprentissage par échantillonnage aléatoire ou pas\n",
    "ind_ech = np.random.choice(adultJoin.index.values, 20000, replace=False)\n",
    "#adultEch=adultJoin.loc[ind_ech]\n",
    "adultEch=adultJoin\n",
    "# Variable cible\n",
    "Y=adultEch[\"income\"]\n",
    "# Variables prédictives\n",
    "X=adultEch.drop([\"income\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.768064Z",
     "start_time": "2020-03-15T07:21:55.197599Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:21:55.773873Z",
     "start_time": "2020-03-15T07:21:55.769797Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prévision du seuil de revenu \n",
    "Une comparaison détaillée (cf. le [tutoriel](https://github.com/wikistat/Apprentissage/blob/master/Adult-Census/Apprent-Python-AdultCensus.ipynb)) de la plupart des modèles et algorithmes de prévision du seuil de revenu en fonction des différentes variables met en évidence des résultats de prévision un peu meilleurs obtenus par l'algorithme de *gradient boosting* (version xtrem). Néanmoins, on peut se limiter ici à un choix plus restreint de modèles et algorithmes pour comprendre l'impact sur la discrimination.\n",
    "- la régression logistique (linéaire) interprétable; \n",
    "- les forêts aléatoires (non linéaire) conduisent à une meilleure précision mais sans capacité d'interprétation,\n",
    "- *gradient boosting* (idem).\n",
    "\n",
    "### 3.1 Prévision par [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "**Q** Comment est opérée la sélection de variables? \n",
    "\n",
    "**Q** Commenter  les options de la commande  `GridSearchCV`. A quoi sert `param` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:30.295962Z",
     "start_time": "2020-03-15T07:21:55.775523Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "tps0=time.perf_counter()\n",
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "param=[{\"C\":[0.9,1,1.1,1.2,1.3,1.4]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver=\"liblinear\"), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui même un estimateur\n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps logit = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                              1.-logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:30.301469Z",
     "start_time": "2020-03-15T07:22:30.298141Z"
    }
   },
   "outputs": [],
   "source": [
    "ClogOpt=logitOpt.best_params_['C'] # pour des étapes ultérieures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:30.338347Z",
     "start_time": "2020-03-15T07:22:30.304472Z"
    }
   },
   "outputs": [],
   "source": [
    "# erreur sur l'échantillon test\n",
    "1-logitOpt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:30.369595Z",
     "start_time": "2020-03-15T07:22:30.340049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prévision\n",
    "y_chap = logitOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quels algorithmes pourraient être exécutés en R pour la régression logistique? \n",
    "\n",
    "**Q** Que dire de l'interprétabilité des résultats par rapport à ceux de R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:30.868996Z",
     "start_time": "2020-03-15T07:22:30.372108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "LogisticRegression(penalty=\"l1\",C=logitOpt.best_params_['C'],\n",
    "                   solver='liblinear').fit(X_train, Y_train).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "En R, il peut être intéressant de construire un arbre binaire de classification. \n",
    "\n",
    "**Q** Pourquoi un arbre fourni par `scikit-learn` présente beaucoup moins d'intérêt?\n",
    "\n",
    "**Q** Commenter les choix de tous les paramètres ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:37.398767Z",
     "start_time": "2020-03-15T07:22:30.870705Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=6, min_samples_leaf=5, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True,n_jobs=-1)\n",
    "# apprentissage\n",
    "rfFit = forest.fit(X_train,Y_train)\n",
    "print(1-rfFit.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:22:37.863240Z",
     "start_time": "2020-03-15T07:22:37.400278Z"
    }
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfFit.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exécution ci-dessous peut êêtre un peu longue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:48.594010Z",
     "start_time": "2020-03-15T07:22:37.867126Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimisation du paramètre\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tps0=time.perf_counter()\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "rf= GridSearchCV(RandomForestClassifier(n_estimators=500,min_samples_split=6, min_samples_leaf=5,n_jobs=-1),param,cv=10,n_jobs=-1)\n",
    "rfOpt=rf.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps r forest = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:48.600541Z",
     "start_time": "2020-03-15T07:29:48.596911Z"
    }
   },
   "outputs": [],
   "source": [
    "CrfOpt=rfOpt.best_params_[\"max_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:49.033037Z",
     "start_time": "2020-03-15T07:29:48.602290Z"
    }
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:49.490760Z",
     "start_time": "2020-03-15T07:29:49.034873Z"
    }
   },
   "outputs": [],
   "source": [
    "# prévision\n",
    "y_chap = rfFit.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:51.867478Z",
     "start_time": "2020-03-15T07:29:49.495270Z"
    }
   },
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators=100,max_features=6, min_samples_leaf=5)\n",
    "rfFit=rf.fit(X_train, Y_train)\n",
    "# Importance décroissante des variables\n",
    "importances = rfFit.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(20):\n",
    "    print(X_train.columns[indices[f]], importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:29:52.085487Z",
     "start_time": "2020-03-15T07:29:51.869519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Graphe des importances\n",
    "plt.figure()\n",
    "plt.title(\"Importances des variables\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment est obtenu le graphique? Quel indicateur d'importance est utilisé? Comment interpréter ces résultats?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [Gradient boosting](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "L'algorithme *machine gradient boosting* pour améliorer les performances.Il est probable que la version XGBoost permette de gagner encore un peu en précision mais ce n'est pas l'objectif ici mais plutôt d'observer l'impact sur le biais de ce type d'algorithme.\n",
    "\n",
    "**Q** Pourquoi pas de paramètre `njobs=-1`? Corrélativement quel est l'intérêt de la librairie XGBoost?\n",
    "\n",
    "**Q** En plus de celui optimisé, quels sont les 2 principaux paramètres de cet algorithme laissés par défaut?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:33:27.650754Z",
     "start_time": "2020-03-15T07:29:52.087384Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "tps0=time.perf_counter()\n",
    "param=[{\"n_estimators\":[300, 350, 400]}]\n",
    "gbm= GridSearchCV(GradientBoostingClassifier(),param,cv=10)\n",
    "gbmOpt=gbm.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps boosting = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                        1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:33:27.654882Z",
     "start_time": "2020-03-15T07:33:27.652399Z"
    }
   },
   "outputs": [],
   "source": [
    "CgbmOpt=gbmOpt.best_params_[\"n_estimators\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:33:27.726597Z",
     "start_time": "2020-03-15T07:33:27.656315Z"
    }
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-gbmOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T07:33:27.802329Z",
     "start_time": "2020-03-15T07:33:27.728568Z"
    }
   },
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = gbmOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le [tutoriel](https://github.com/wikistat/Apprentissage/blob/master/Adult-Census/Apprent-Python-AdultCensus.ipynb) du dépôt d'apprentissage compare finement les performances des algoritmes: courbes ROC et [Validation croisée](http://wikistat.fr/pdf/st-m-app-risque.pdf) *Monte Carlo*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Détection des discriminations\n",
    "Cette section est focalisée sur les codes des fonctions d'estimations par intervalle de confiance des principaux indicateurs de biais. Un travail laissé en exercice permettrait d'obtenir facilement les résultats complémentaires du [tutoriel en R](https://github.com/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-R-biasDetectionCourt.ipynb): suppression de la variable sensible du modèle, inutilité du *testing*, atténuation élémentaire du biais et discrimination positive, effet de cette atténuation sur les autres biais.\n",
    "\n",
    "### 4.1 Estimation de l'effet disproportionné\n",
    "Fontions d'estimation pontuelle et par intervalle de confiance de l'effet disproportionné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T11:09:21.641782Z",
     "start_time": "2020-03-15T11:09:21.632408Z"
    }
   },
   "outputs": [],
   "source": [
    "def DI(tab):\n",
    "    \"\"\"\n",
    "    Calcule le disparate impact ponctuel d'une table de contingence \n",
    "    Colonnes = Variable sensible\n",
    "    Lignes = variable cible\n",
    "    \"\"\"\n",
    "    tab_np = tab.to_numpy()\n",
    "    DI = (tab_np[0,0] / (tab_np[1,0] + tab_np[0,0])) / (tab_np[0,1] / (tab_np[1,1] + tab_np[0,1]))\n",
    "    return DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispImp(var_sens, var_cible, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Estimation du disparate impact par intervalle de confiance  \n",
    "                                                             \n",
    "  Cette fonction prend trois arguments en entrée :           \n",
    "    - La variable binaire considérée comme sensible          \n",
    "    - La variable cible Y ou sa prévision                    \n",
    "    - alpha par défaut 0.05                                  \n",
    "  Elle renvoie la valeur Tn du disparate impact et des bornes de l'intervalle de confiance                \n",
    "  Attention à l'ordre lexicographique des niveaux des facteurs, le premier est par convention celui jugé défavorable  \n",
    "    \"\"\"\n",
    "\n",
    "    S = var_sens.astype('int')\n",
    "    Y = var_cible.astype('int')\n",
    "    n = len(S)\n",
    "    \n",
    "    pi_1 = np.sum(S)/n #estimated P(S=1)\n",
    "    pi_0 = 1 - pi_1 #estimated P(S=0)\n",
    "    p_1 = np.sum(S * Y) / n #estimated P(g(X)=1, S=1)\n",
    "    p_0 = np.sum((1 - S) * Y) / n #estimated P(g(X)=1, S=0)\n",
    "    Tn = p_0 * pi_1 / (p_1 * pi_0) # disparate impact\n",
    "    grad_h = lambda x: np.array((x[3]/(x[1]*x[2]), -x[0]*x[3]/(x[1]**2 * x[2]), -x[0]*x[3]/(x[2]**2 * x[1]), x[0]/(x[2]*x[1])))\n",
    "    grad = grad_h([p_0, p_1, pi_0, pi_1])\n",
    "    Cov_4 = np.array([(0, -p_0*p_1, pi_1*p_0, -pi_1*p_0), (0, 0, -pi_0*p_1, pi_0*p_1), (0,0,0,-pi_0*pi_1), (0,0,0,0)]).T\n",
    "    Cov_4 = Cov_4 + Cov_4.T + np.diag((p_0*(1-p_0), p_1*(1-p_1),pi_0*pi_1,pi_0*pi_1))\n",
    "    sigma = np.sqrt((grad.dot(Cov_4)).dot(grad))\n",
    "    \n",
    "    lower_lim = Tn - (sigma * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    upper_lim = Tn + (sigma * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    \n",
    "    return np.array([lower_lim, Tn, upper_lim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T11:09:24.077781Z",
     "start_time": "2020-03-15T11:09:23.523772Z"
    }
   },
   "outputs": [],
   "source": [
    "logitFixe = LogisticRegression(penalty=\"l1\",solver=\"liblinear\", C=ClogOpt)\n",
    "logitFixe.fit(X_train, Y_train)\n",
    "y_chap_logit = logitFixe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T11:09:40.357421Z",
     "start_time": "2020-03-15T11:09:26.157352Z"
    }
   },
   "outputs": [],
   "source": [
    "rfFixe = RandomForestClassifier(n_estimators=500, max_features=CrfOpt)\n",
    "rfFixe.fit(X_train, Y_train)\n",
    "y_chap_rf = rfFixe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.178311Z",
     "start_time": "2020-03-15T09:33:45.099843Z"
    }
   },
   "outputs": [],
   "source": [
    "gbmFixe = GradientBoostingClassifier(n_estimators=CgbmOpt)\n",
    "gbmFixe.fit(X_train, Y_train)\n",
    "y_chap_gbm = gbmFixe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.220231Z",
     "start_time": "2020-03-15T09:33:55.179818Z"
    }
   },
   "outputs": [],
   "source": [
    "# récupération des matrices de confusion\n",
    "tableSex_logit = pd.crosstab(y_chap_logit, X_test['sex_ Male'].map({0: \"Female\", 1: \"Male\"}),\n",
    "                          rownames=['Sex'], colnames=['Inc'])\n",
    "tableSex_rf = pd.crosstab(y_chap_rf, X_test['sex_ Male'].map({0: \"Female\", 1: \"Male\"}), \n",
    "                          rownames=['Sex'], colnames=['Inc'])\n",
    "tableSex_gbm = pd.crosstab(y_chap_gbm, X_test['sex_ Male'].map({0: \"Female\", 1: \"Male\"}), \n",
    "                          rownames=['Sex'], colnames=['Inc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.226055Z",
     "start_time": "2020-03-15T09:33:55.221829Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calcul des DI ponctuels\n",
    "diSex_logit = DI(tableSex_logit)\n",
    "diSex_rf = DI(tableSex_rf)\n",
    "diSex_gbm = DI(tableSex_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.233433Z",
     "start_time": "2020-03-15T09:33:55.228132Z"
    }
   },
   "outputs": [],
   "source": [
    "print(diSex_logit)\n",
    "print(diSex_rf)\n",
    "print(diSex_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.244805Z",
     "start_time": "2020-03-15T09:33:55.235575Z"
    }
   },
   "outputs": [],
   "source": [
    "dispimpSex_theorique = dispImp(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.261627Z",
     "start_time": "2020-03-15T09:33:55.246964Z"
    }
   },
   "outputs": [],
   "source": [
    "dispimpSex_logit = dispImp(X_test[\"sex_ Male\"].values, \n",
    "                            pd.Series(y_chap_logit).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "dispimpSex_rf = dispImp(X_test[\"sex_ Male\"].values, \n",
    "                         pd.Series(y_chap_rf).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "dispimpSex_gbm = dispImp(X_test[\"sex_ Male\"].values, \n",
    "                          pd.Series(y_chap_gbm).map({\"incLow\": 0, \"incHigh\": 1}).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.268473Z",
     "start_time": "2020-03-15T09:33:55.263474Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dispimpSex_theorique)\n",
    "print(dispimpSex_logit)\n",
    "print(dispimpSex_rf)\n",
    "print(dispimpSex_gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparer ces intervalles de confiance et avec ceux obtenus en R. Commenter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation graphique\n",
    "Le graphique obtenu est très sommaire, il mériterait un peu de travail afin de retrouver une présentation plus explicite comme celle obtenue avec les fonctions R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T09:33:55.485873Z",
     "start_time": "2020-03-15T09:33:55.269890Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20) \n",
    "\n",
    "# Create a dataframe\n",
    "IC_l = [x[0] for x in [dispimpSex_theorique, dispimpSex_logit, dispimpSex_rf, dispimpSex_gbm]]\n",
    "mean = [x[1] for x in [dispimpSex_theorique, dispimpSex_logit, dispimpSex_rf, dispimpSex_gbm]]\n",
    "IC_h = [x[2] for x in [dispimpSex_theorique, dispimpSex_logit, dispimpSex_rf, dispimpSex_gbm]]\n",
    "\n",
    "df = pd.DataFrame({'method': list(map(str, [\"données\", \"logit\", \"rf\", \"gbm\"])), \n",
    "                   'IC_l' : IC_l , 'mean' : mean, 'IC_h' : IC_h })\n",
    "\n",
    "# Reorder it following the values of the first value:\n",
    "\n",
    "my_range=range(1,len(df.index)+1)\n",
    "col = [ \"slateblue\",  \"cadetblue\", \"darkolivegreen\", \"y\"]\n",
    "plt.hlines(y=my_range, xmin=df['IC_l'], xmax=df['IC_h'], color='darkblue', alpha=0.8, linewidth=4)\n",
    "plt.scatter(df['IC_l'], my_range, color='darkblue', alpha=0.8, marker=4)\n",
    "plt.scatter(df['mean'], my_range, color=col, alpha=1, marker=\"s\", linewidths=8)\n",
    "plt.scatter(df['IC_h'], my_range, color='darkblue', alpha=0.8, marker=5)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim((0.1, 0.6))\n",
    "# plt.xlim((0.1, 0.7))\n",
    "# Add title and axis names\n",
    "plt.yticks(my_range, df['method'])\n",
    "plt.title(\"Effet disproportionné du genre selon diverses approches\", loc='center')\n",
    "plt.xlabel('Effet Disproportionné')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparaison des erreurs de prévision\n",
    "Estimation par intervalle de confiance du rapport des erreurs conditionelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overErrEqual(var_sens, var_cible, var_prev, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Estimation de \"overall Error Equality\"   par Int. de Conf.  \n",
    "                                                             \n",
    "  Cette fonction prend trois arguments en entrée :           \n",
    "    - La variable binaire considérée comme sensible          \n",
    "    - La variable cible Y                                    \n",
    "    - La prévision P de cette variable                       \n",
    "    - alpha par défaut 0.05                                  \n",
    "  Elle renvoie la valeur du rapport des erreurs              \n",
    "   et des bornes de l'intervalle de confiance                \n",
    " Attention à l'ordre lexicographique des niveaux des facteurs\n",
    " Le premier est par convention celui jugé défavorable\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_h = lambda x: np.array((x[3]/(x[1]*x[2]), -x[0]*x[3]/(x[1]**2 * x[2]), -x[0]*x[3]/(x[2]**2 * x[1]), x[0]/(x[2]*x[1])))\n",
    "    \n",
    "    S = var_sens.astype('int')\n",
    "    Y = var_cible.astype('int')\n",
    "    P = var_prev.astype('int')\n",
    "    n = len(S)\n",
    "    \n",
    "    pi_1 = np.sum(S)/n #estimated P(S=1)\n",
    "    pi_0 = 1-pi_1 #estimated P(S=0)\n",
    "    \n",
    "    # Testing Overall Error Equality: P(g(X,S)\\neq Y|S=0)/P(g(X,S)\\neq Y|S=1) \n",
    "    pp_0 = np.sum((1-S)*(1-Y)*P)/n #estimated P(g(X,S)=1, Y=0, S=0)\n",
    "    qq_0 = np.sum((1-S)*Y*(1-P))/n #estimated P(g(X,S)=0, Y=1, S=0)\n",
    "    pp_1 = np.sum(S*(1-Y)*P)/n #estimated P(g(X,S)=1, Y=0, S=1)\n",
    "    qq_1 = np.sum(S*Y*(1-P))/n #estimated P(g(X,S)=0, Y=1, S=1)\n",
    "\n",
    "    TnA3 = (pp_0+qq_0)*pi_1/((pp_1+qq_1)*pi_0) #statistic\n",
    "    grad = grad_h([pp_0+qq_0,pp_1+qq_1,pi_0,pi_1])\n",
    "    \n",
    "    Cov_4 = np.array([(0, -(pp_0+qq_0)*(pp_1+qq_1), (pp_0+qq_0)*(1-pi_0), -(pp_0+qq_0)*pi_1), (0, 0, -(pp_1+qq_1)*pi_0, (pp_1+qq_1)*(1-pi_1)), (0,0,0,-pi_0*pi_1), (0,0,0,0)])\n",
    "    Cov_4 = Cov_4 + Cov_4.T + np.diag(((pp_0+qq_0)*(1-(pp_0+qq_0)), (pp_1+qq_1)*(1-(pp_1+qq_1)),pi_0*(1-pi_0),pi_1*(1-pi_1)))\n",
    "    sigmaA3 = np.sqrt((grad.dot(Cov_4)).dot(grad))\n",
    "    lower_limA3 = TnA3 - (sigmaA3 * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    upper_limA3 = TnA3 + (sigmaA3 * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    \n",
    "    OEE_CI = pd.DataFrame({\"inf\": lower_limA3,\n",
    "                              \"est_value\": [TnA3],\n",
    "                              \"sup\": [upper_limA3]},\n",
    "                             index=[\"OEE\"])\n",
    "    return OEE_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errSex_logit = overErrEqual(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_logit).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "errSex_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errSex_rf = overErrEqual(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_rf).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "errSex_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errSex_gbm = overErrEqual(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_gbm).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "errSex_gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparaison des cotes\n",
    "Estiation par intervalle de confiance des rapports des cotes conditionnelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalOdds(var_sens, var_cible, var_prev, alpha=0.05):\n",
    "    \"\"\"\n",
    "     Estimation de equalized odds par intervalle de confiance  \n",
    "FPRS: P(g(X,S)=1|Y=0,S=0)/P(g(X,S)=1|Y=0,S=1)               \n",
    "TPRs: P(g(X,S)=1|Y=1,S=0)=P(g(X,S)=1|Y=1,S=1)               \n",
    "Cette fonction prend trois arguments en entrée :           \n",
    "    - La variable binaire considérée comme sensible          \n",
    "    - La variable cible Y observée                           \n",
    "    - La prévision de P de Y                                 \n",
    "    - alpha par défaut 0.05                                  \n",
    "  Elle renvoie la valeur Tn du disparate impact              \n",
    "   et des bornes de l'intervalle de confiance                \n",
    " Attention à l'ordre lexicographique des niveaux des facteurs\n",
    " Le premier est par convention celui jugé défavorable        \n",
    "    \"\"\"\n",
    "    \n",
    "    S = var_sens.astype('int')\n",
    "    Y = var_cible.astype('int')\n",
    "    P = var_prev.astype('int')\n",
    "    n = len(S)\n",
    "    \n",
    "    ### 1.- Testing FPRs equality\n",
    "    p_1 = np.sum(S*(1-Y)*P)/n #estimated P(g(X)=1, Y=0, S=1)\n",
    "    p_0 = np.sum((1-S)*(1-Y)*P)/n #estimated P(g(X)=1, Y=0, S=0)\n",
    "    r_1 = np.sum(S*(1-Y))/n #estimated P(Y=0, S=1)\n",
    "    r_0 = np.sum((1-S)*(1-Y))/n #estimated P(Y=0, S=0)\n",
    "    \n",
    "    FPRR = p_0*r_1/(p_1*r_0) #statistic FALSE POSITIVE RATE\n",
    "    grad_h = lambda x: np.array((x[3]/(x[1]*x[2]), -x[0]*x[3]/(x[1]**2 * x[2]), -x[0]*x[3]/(x[2]**2 * x[1]), x[0]/(x[2]*x[1])))\n",
    "    grad = grad_h([p_0, p_1, r_0, r_1])\n",
    "    \n",
    "    Cov_4 = np.array([(0, -p_0*p_1, p_0*(1-r_0), -p_0*r_1), (0, 0, -p_1*r_0, p_1*(1-r_1)), (0,0,0,-r_0*r_1), (0,0,0,0)]).T\n",
    "    Cov_4 = Cov_4 + Cov_4.T + np.diag((p_0*(1-p_0), p_1*(1-p_1),r_0*(1-r_0),r_1*(1-r_1)))\n",
    "    sigma = np.sqrt((grad.dot(Cov_4)).dot(grad))\n",
    "    lower_lim1 = FPRR - (sigma * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    upper_lim1 = FPRR + (sigma * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    \n",
    "    \n",
    "    ### 2.- Testing TPRs equality: P(g(X,S)=1|Y=1,S=0)/P(g(X,S)=1|Y=1,S=1)\n",
    "    q_1 = np.sum(S*Y*P)/n #estimated P(g(X,S)=1, Y=1, S=1)\n",
    "    q_0 = np.sum((1-S)*Y*P)/n #estimated P(g(X,S)=1, Y=1, S=0)\n",
    "    s_1 = np.sum(S*Y)/n #estimated P(Y=1, S=1)\n",
    "    s_0 = np.sum((1-S)*Y)/n #estimated P(Y=1, S=0)\n",
    "    \n",
    "    TPRR = q_0*s_1/(q_1*s_0) #statistic TRUE POSITIVE\n",
    "    grad = grad_h([q_0,q_1,s_0,s_1])\n",
    "    \n",
    "    Cov_4 = np.array([(0, -q_0*q_1, q_0*(1-s_0), -q_0*s_1), (0, 0, -q_1*s_0, q_1*(1-s_1)), (0,0,0,-s_0*s_1), (0,0,0,0)]).T\n",
    "    Cov_4 = Cov_4 + Cov_4.T + np.diag((q_0*(1-q_0), q_1*(1-q_1),s_0*(1-s_0),s_1*(1-s_1)))\n",
    "    sigmaA1 = np.sqrt((grad.dot(Cov_4)).dot(grad))\n",
    "    lower_lim2 = TPRR - (sigmaA1 * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    upper_lim2 = TPRR + (sigmaA1 * scipy.stats.norm.ppf(1-alpha/2)) / np.sqrt(n)\n",
    "    \n",
    "    EQODDS_CI = pd.DataFrame({\"inf\": [lower_lim1, lower_lim2],\n",
    "                              \"est_value\": [FPRR, TPRR],\n",
    "                              \"sup\": [upper_lim1, upper_lim2]},\n",
    "                             index=[\"FPR\", \"TPR\"])\n",
    "    \n",
    "    return EQODDS_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddsSex_logit = equalOdds(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_logit).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "oddsSex_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddsSex_rf = equalOdds(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_rf).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "oddsSex_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddsSex_gbm = equalOdds(X_test[\"sex_ Male\"].values, Y_test.map({\"incLow\": 0, \"incHigh\": 1}).values, \n",
    "                            pd.Series(y_chap_gbm).map({\"incLow\": 0, \"incHigh\": 1}).values)\n",
    "oddsSex_gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chantier** \n",
    "\n",
    "La question de l'atténuation du biais serait à compléter afin de répondre à la question: les approches complexes de la littérature, comparées par Frieder et al. (2019) ou référencées sur le site [IBM research](https://aif360.mybluemix.net/) sont elles, au regard des coûts de calcul, plus efficaces que la version rudimentaire de post processing  développée dans le [tutoriel en R](https://github.com/wikistat/Fair-ML-4-Ethical-AI/blob/master/AdultCensus/AdultCensus-R-biasDetection.ipynb) mais pas reprises ici?\n",
    "\n",
    "**Référence**\n",
    "\n",
    "Friedler S., Scheidegger C., Venkatasubramanian S., Choudhary S., Hamilton E., Roth D. (2019). [Comparative study of fairness-enhancing interventions in machine learning](http://dl.acm.org/citation.cfm?doid=3287560.3287589). Proceedings of the Conference on Fairness, Accountability, and Transparency, p. 329‐38. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
